{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Make sure that caffe is on the python path:\n",
    "caffe_root = 'caffe/'\n",
    "import sys\n",
    "sys.path.insert(0, caffe_root + 'python')\n",
    "\n",
    "import caffe\n",
    "\n",
    "import os\n",
    "import h5py\n",
    "import shutil\n",
    "import tempfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "X = []\n",
    "with open('Data/Train/data.bin', 'r') as f:\n",
    "    while True:\n",
    "        try:\n",
    "            X.append(np.load(f))\n",
    "        except Exception, e:\n",
    "            break\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "h = 30\n",
    "w = 30\n",
    "Xt_resized = []\n",
    "Xt = []\n",
    "with open('Data/Test/data.bin', 'r') as f:\n",
    "    while True:\n",
    "        try:\n",
    "            Xt.append(np.load(f))\n",
    "            Xt_resized.append(scipy.misc.imresize(Xt[-1], (h, w)))\n",
    "        except Exception, e:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f4df9814b10>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAASkAAAD+CAYAAABm1fOhAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f4df988f310>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(X[0], cmap=plt.cm.gray)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "70000\n",
      "80000\n",
      "90000\n",
      "100000\n",
      "110000\n",
      "120000\n",
      "130000\n",
      "140000\n",
      "150000\n",
      "160000\n",
      "170000\n",
      "180000\n",
      "190000\n"
     ]
    }
   ],
   "source": [
    "import scipy\n",
    "import scipy.misc\n",
    "h = 30\n",
    "w = 30\n",
    "X_resized = []\n",
    "for (index, image) in enumerate(X):\n",
    "    if index % 10000 == 0:\n",
    "        print index\n",
    "    X_resized.append(scipy.misc.imresize(X[index], (h, w)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Xt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-33-dac819e48a04>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mw\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m30\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mXt_resized\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[1;32mfor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimage\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mXt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mindex\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;36m10000\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m         \u001b[1;32mprint\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'Xt' is not defined"
     ]
    }
   ],
   "source": [
    "h = 30\n",
    "w = 30\n",
    "Xt_resized = []\n",
    "for (index, image) in enumerate(Xt):\n",
    "    if index % 10000 == 0:\n",
    "        print index\n",
    "    Xt_resized.append(scipy.misc.imresize(Xt[index], (h, w)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "y = pd.read_csv('Data/Train/train.csv')\n",
    "y = y['Prediction'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_resized = np.array(X_resized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_blind = np.array(Xt_resized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_samples = len(X)\n",
    "n_train_samples = int(0.9 * n_samples)\n",
    "\n",
    "indices = np.arange(n_samples)\n",
    "np.random.shuffle(indices)\n",
    "\n",
    "train_indices = indices[: n_train_samples]\n",
    "test_indices = indices[n_train_samples:]\n",
    "X_train = X_resized[train_indices, :]\n",
    "y_train = y[train_indices]\n",
    "X_test = X_resized[test_indices, :]\n",
    "y_test = y[test_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(180000, 30, 30)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Write out the data to HDF5 files in a temp directory.\n",
    "# This file is assumed to be caffe_root/examples/hdf5_classification.ipynb\n",
    "dirname = os.path.abspath('./rdlclass/data')\n",
    "if not os.path.exists(dirname):\n",
    "    os.makedirs(dirname)\n",
    "\n",
    "train_filename = os.path.join(dirname, 'train.h5')\n",
    "test_filename = os.path.join(dirname, 'test.h5')\n",
    "blind_filename = os.path.join(dirname, 'blind.h5')\n",
    "\n",
    "# HDF5DataLayer source should be a file containing a list of HDF5 filenames.\n",
    "# To show this off, we'll list the same data file twice.\n",
    "with h5py.File(train_filename, 'w') as f:\n",
    "    f['data'] = X_train.astype(np.float32)\n",
    "    f['label'] = y_train.astype(np.float32)\n",
    "with open(os.path.join(dirname, 'train.txt'), 'w') as f:\n",
    "    f.write(train_filename + '\\n')\n",
    "    \n",
    "# HDF5 is pretty efficient, but can be further compressed.\n",
    "comp_kwargs = {'compression': 'gzip', 'compression_opts': 1}\n",
    "with h5py.File(test_filename, 'w') as f:\n",
    "    f.create_dataset('data', data=X_test.astype(np.float32), **comp_kwargs)\n",
    "    f.create_dataset('label', data=y_test.astype(np.float32), **comp_kwargs)\n",
    "with open(os.path.join(dirname, 'test.txt'), 'w') as f:\n",
    "    f.write(test_filename + '\\n')\n",
    "    \n",
    "# HDF5 is pretty efficient, but can be further compressed.\n",
    "#comp_kwargs = {'compression': 'gzip', 'compression_opts': 1}\n",
    "#with h5py.File(blind_filename, 'w') as f:\n",
    "#    f.create_dataset('data', data=X_blind.astype(np.float32), **comp_kwargs)\n",
    "#with open(os.path.join(dirname, 'blind.txt'), 'w') as f:\n",
    "#    f.write(blind_filename + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "libdc1394 error: Failed to initialize libdc1394\n",
      "I0411 16:22:46.620281 16153 caffe.cpp:117] Use CPU.\n",
      "I0411 16:22:46.620590 16153 caffe.cpp:121] Starting Optimization\n",
      "I0411 16:22:46.620708 16153 solver.cpp:32] Initializing solver from parameters: \n",
      "test_iter: 250\n",
      "test_interval: 1000\n",
      "base_lr: 0.01\n",
      "display: 100\n",
      "max_iter: 10000\n",
      "lr_policy: \"step\"\n",
      "gamma: 0.1\n",
      "momentum: 0.9\n",
      "weight_decay: 0.0005\n",
      "stepsize: 5000\n",
      "snapshot: 10000\n",
      "snapshot_prefix: \"rdlclass/data/train\"\n",
      "solver_mode: CPU\n",
      "net: \"rdlclass/train_val.prototxt\"\n",
      "I0411 16:22:46.620779 16153 solver.cpp:70] Creating training net from net file: rdlclass/train_val.prototxt\n",
      "I0411 16:22:46.621148 16153 net.cpp:257] The NetState phase (0) differed from the phase (1) specified by a rule in layer data\n",
      "I0411 16:22:46.621183 16153 net.cpp:257] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy\n",
      "I0411 16:22:46.621266 16153 net.cpp:42] Initializing net from parameters: \n",
      "name: \"LogisticRegressionNet\"\n",
      "state {\n",
      "  phase: TRAIN\n",
      "}\n",
      "layer {\n",
      "  name: \"data\"\n",
      "  type: \"HDF5Data\"\n",
      "  top: \"data\"\n",
      "  top: \"label\"\n",
      "  include {\n",
      "    phase: TRAIN\n",
      "  }\n",
      "  hdf5_data_param {\n",
      "    source: \"rdlclass/data/train.txt\"\n",
      "    batch_size: 500\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"fc1\"\n",
      "  type: \"InnerProduct\"\n",
      "  bottom: \"data\"\n",
      "  top: \"fc1\"\n",
      "  param {\n",
      "    lr_mult: 1\n",
      "    decay_mult: 1\n",
      "  }\n",
      "  param {\n",
      "    lr_mult: 2\n",
      "    decay_mult: 0\n",
      "  }\n",
      "  inner_product_param {\n",
      "    num_output: 2000\n",
      "    weight_filler {\n",
      "      type: \"gaussian\"\n",
      "      std: 0.01\n",
      "    }\n",
      "    bias_filler {\n",
      "      type: \"constant\"\n",
      "      value: 0\n",
      "    }\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"loss\"\n",
      "  type: \"SoftmaxWithLoss\"\n",
      "  bottom: \"fc1\"\n",
      "  bottom: \"label\"\n",
      "  top: \"loss\"\n",
      "}\n",
      "I0411 16:22:46.621611 16153 layer_factory.hpp:74] Creating layer data\n",
      "I0411 16:22:46.621665 16153 net.cpp:84] Creating Layer data\n",
      "I0411 16:22:46.621701 16153 net.cpp:338] data -> data\n",
      "I0411 16:22:46.621760 16153 net.cpp:338] data -> label\n",
      "I0411 16:22:46.621798 16153 net.cpp:113] Setting up data\n",
      "I0411 16:22:46.621827 16153 hdf5_data_layer.cpp:80] Loading list of HDF5 filenames from: rdlclass/data/train.txt\n",
      "I0411 16:22:46.621932 16153 hdf5_data_layer.cpp:94] Number of HDF5 files: 1\n",
      "I0411 16:22:47.356319 16153 net.cpp:120] Top shape: 500 30 30 (450000)\n",
      "I0411 16:22:47.356379 16153 net.cpp:120] Top shape: 500 (500)\n",
      "I0411 16:22:47.356407 16153 layer_factory.hpp:74] Creating layer fc1\n",
      "I0411 16:22:47.356498 16153 net.cpp:84] Creating Layer fc1\n",
      "I0411 16:22:47.356519 16153 net.cpp:380] fc1 <- data\n",
      "I0411 16:22:47.356546 16153 net.cpp:338] fc1 -> fc1\n",
      "I0411 16:22:47.356570 16153 net.cpp:113] Setting up fc1\n",
      "I0411 16:22:47.415083 16153 net.cpp:120] Top shape: 500 2000 (1000000)\n",
      "I0411 16:22:47.415159 16153 layer_factory.hpp:74] Creating layer loss\n",
      "I0411 16:22:47.415233 16153 net.cpp:84] Creating Layer loss\n",
      "I0411 16:22:47.415264 16153 net.cpp:380] loss <- fc1\n",
      "I0411 16:22:47.415294 16153 net.cpp:380] loss <- label\n",
      "I0411 16:22:47.415318 16153 net.cpp:338] loss -> loss\n",
      "I0411 16:22:47.415346 16153 net.cpp:113] Setting up loss\n",
      "I0411 16:22:47.415365 16153 layer_factory.hpp:74] Creating layer loss\n",
      "I0411 16:22:47.418231 16153 net.cpp:120] Top shape: (1)\n",
      "I0411 16:22:47.418262 16153 net.cpp:122]     with loss weight 1\n",
      "I0411 16:22:47.418318 16153 net.cpp:167] loss needs backward computation.\n",
      "I0411 16:22:47.418339 16153 net.cpp:167] fc1 needs backward computation.\n",
      "I0411 16:22:47.418354 16153 net.cpp:169] data does not need backward computation.\n",
      "I0411 16:22:47.418364 16153 net.cpp:205] This network produces output loss\n",
      "I0411 16:22:47.418376 16153 net.cpp:447] Collecting Learning Rate and Weight Decay.\n",
      "I0411 16:22:47.418388 16153 net.cpp:217] Network initialization done.\n",
      "I0411 16:22:47.418397 16153 net.cpp:218] Memory required for data: 5802004\n",
      "I0411 16:22:47.418643 16153 solver.cpp:154] Creating test net (#0) specified by net file: rdlclass/train_val.prototxt\n",
      "I0411 16:22:47.418694 16153 net.cpp:257] The NetState phase (1) differed from the phase (0) specified by a rule in layer data\n",
      "I0411 16:22:47.418797 16153 net.cpp:42] Initializing net from parameters: \n",
      "name: \"LogisticRegressionNet\"\n",
      "state {\n",
      "  phase: TEST\n",
      "}\n",
      "layer {\n",
      "  name: \"data\"\n",
      "  type: \"HDF5Data\"\n",
      "  top: \"data\"\n",
      "  top: \"label\"\n",
      "  include {\n",
      "    phase: TEST\n",
      "  }\n",
      "  hdf5_data_param {\n",
      "    source: \"rdlclass/data/test.txt\"\n",
      "    batch_size: 10\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"fc1\"\n",
      "  type: \"InnerProduct\"\n",
      "  bottom: \"data\"\n",
      "  top: \"fc1\"\n",
      "  param {\n",
      "    lr_mult: 1\n",
      "    decay_mult: 1\n",
      "  }\n",
      "  param {\n",
      "    lr_mult: 2\n",
      "    decay_mult: 0\n",
      "  }\n",
      "  inner_product_param {\n",
      "    num_output: 2000\n",
      "    weight_filler {\n",
      "      type: \"gaussian\"\n",
      "      std: 0.01\n",
      "    }\n",
      "    bias_filler {\n",
      "      type: \"constant\"\n",
      "      value: 0\n",
      "    }\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"loss\"\n",
      "  type: \"SoftmaxWithLoss\"\n",
      "  bottom: \"fc1\"\n",
      "  bottom: \"label\"\n",
      "  top: \"loss\"\n",
      "}\n",
      "layer {\n",
      "  name: \"accuracy\"\n",
      "  type: \"Accuracy\"\n",
      "  bottom: \"fc1\"\n",
      "  bottom: \"label\"\n",
      "  top: \"accuracy\"\n",
      "  include {\n",
      "    phase: TEST\n",
      "  }\n",
      "}\n",
      "I0411 16:22:47.419102 16153 layer_factory.hpp:74] Creating layer data\n",
      "I0411 16:22:47.419128 16153 net.cpp:84] Creating Layer data\n",
      "I0411 16:22:47.419139 16153 net.cpp:338] data -> data\n",
      "I0411 16:22:47.419153 16153 net.cpp:338] data -> label\n",
      "I0411 16:22:47.419165 16153 net.cpp:113] Setting up data\n",
      "I0411 16:22:47.419179 16153 hdf5_data_layer.cpp:80] Loading list of HDF5 filenames from: rdlclass/data/test.txt\n",
      "I0411 16:22:47.419234 16153 hdf5_data_layer.cpp:94] Number of HDF5 files: 1\n",
      "I0411 16:22:47.922739 16153 net.cpp:120] Top shape: 10 30 30 (9000)\n",
      "I0411 16:22:47.922796 16153 net.cpp:120] Top shape: 10 (10)\n",
      "I0411 16:22:47.922821 16153 layer_factory.hpp:74] Creating layer label_data_1_split\n",
      "I0411 16:22:47.922906 16153 net.cpp:84] Creating Layer label_data_1_split\n",
      "I0411 16:22:47.922929 16153 net.cpp:380] label_data_1_split <- label\n",
      "I0411 16:22:47.922960 16153 net.cpp:338] label_data_1_split -> label_data_1_split_0\n",
      "I0411 16:22:47.922981 16153 net.cpp:338] label_data_1_split -> label_data_1_split_1\n",
      "I0411 16:22:47.923004 16153 net.cpp:113] Setting up label_data_1_split\n",
      "I0411 16:22:47.923023 16153 net.cpp:120] Top shape: 10 (10)\n",
      "I0411 16:22:47.923034 16153 net.cpp:120] Top shape: 10 (10)\n",
      "I0411 16:22:47.923044 16153 layer_factory.hpp:74] Creating layer fc1\n",
      "I0411 16:22:47.923064 16153 net.cpp:84] Creating Layer fc1\n",
      "I0411 16:22:47.923089 16153 net.cpp:380] fc1 <- data\n",
      "I0411 16:22:47.923110 16153 net.cpp:338] fc1 -> fc1\n",
      "I0411 16:22:47.923137 16153 net.cpp:113] Setting up fc1\n",
      "I0411 16:22:47.982053 16153 net.cpp:120] Top shape: 10 2000 (20000)\n",
      "I0411 16:22:47.982118 16153 layer_factory.hpp:74] Creating layer fc1_fc1_0_split\n",
      "I0411 16:22:47.982156 16153 net.cpp:84] Creating Layer fc1_fc1_0_split\n",
      "I0411 16:22:47.982178 16153 net.cpp:380] fc1_fc1_0_split <- fc1\n",
      "I0411 16:22:47.982203 16153 net.cpp:338] fc1_fc1_0_split -> fc1_fc1_0_split_0\n",
      "I0411 16:22:47.982230 16153 net.cpp:338] fc1_fc1_0_split -> fc1_fc1_0_split_1\n",
      "I0411 16:22:47.982254 16153 net.cpp:113] Setting up fc1_fc1_0_split\n",
      "I0411 16:22:47.982270 16153 net.cpp:120] Top shape: 10 2000 (20000)\n",
      "I0411 16:22:47.982280 16153 net.cpp:120] Top shape: 10 2000 (20000)\n",
      "I0411 16:22:47.982290 16153 layer_factory.hpp:74] Creating layer loss\n",
      "I0411 16:22:47.982306 16153 net.cpp:84] Creating Layer loss\n",
      "I0411 16:22:47.982316 16153 net.cpp:380] loss <- fc1_fc1_0_split_0\n",
      "I0411 16:22:47.982326 16153 net.cpp:380] loss <- label_data_1_split_0\n",
      "I0411 16:22:47.982338 16153 net.cpp:338] loss -> loss\n",
      "I0411 16:22:47.982352 16153 net.cpp:113] Setting up loss\n",
      "I0411 16:22:47.982368 16153 layer_factory.hpp:74] Creating layer loss\n",
      "I0411 16:22:47.982426 16153 net.cpp:120] Top shape: (1)\n",
      "I0411 16:22:47.982450 16153 net.cpp:122]     with loss weight 1\n",
      "I0411 16:22:47.982484 16153 layer_factory.hpp:74] Creating layer accuracy\n",
      "I0411 16:22:47.982513 16153 net.cpp:84] Creating Layer accuracy\n",
      "I0411 16:22:47.982527 16153 net.cpp:380] accuracy <- fc1_fc1_0_split_1\n",
      "I0411 16:22:47.982538 16153 net.cpp:380] accuracy <- label_data_1_split_1\n",
      "I0411 16:22:47.982550 16153 net.cpp:338] accuracy -> accuracy\n",
      "I0411 16:22:47.982565 16153 net.cpp:113] Setting up accuracy\n",
      "I0411 16:22:47.982585 16153 net.cpp:120] Top shape: (1)\n",
      "I0411 16:22:47.982596 16153 net.cpp:169] accuracy does not need backward computation.\n",
      "I0411 16:22:47.982606 16153 net.cpp:167] loss needs backward computation.\n",
      "I0411 16:22:47.982616 16153 net.cpp:167] fc1_fc1_0_split needs backward computation.\n",
      "I0411 16:22:47.982630 16153 net.cpp:167] fc1 needs backward computation.\n",
      "I0411 16:22:47.982713 16153 net.cpp:169] label_data_1_split does not need backward computation.\n",
      "I0411 16:22:47.982735 16153 net.cpp:169] data does not need backward computation.\n",
      "I0411 16:22:47.982753 16153 net.cpp:205] This network produces output accuracy\n",
      "I0411 16:22:47.982764 16153 net.cpp:205] This network produces output loss\n",
      "I0411 16:22:47.982779 16153 net.cpp:447] Collecting Learning Rate and Weight Decay.\n",
      "I0411 16:22:47.982791 16153 net.cpp:217] Network initialization done.\n",
      "I0411 16:22:47.982800 16153 net.cpp:218] Memory required for data: 276128\n",
      "I0411 16:22:47.982849 16153 solver.cpp:42] Solver scaffolding done.\n",
      "I0411 16:22:47.982874 16153 solver.cpp:222] Solving LogisticRegressionNet\n",
      "I0411 16:22:47.982892 16153 solver.cpp:223] Learning Rate Policy: step\n",
      "I0411 16:22:47.982919 16153 solver.cpp:266] Iteration 0, Testing net (#0)\n",
      "I0411 16:22:49.119405 16153 solver.cpp:315]     Test net output #0: accuracy = 0.0012\n",
      "I0411 16:22:49.119503 16153 solver.cpp:315]     Test net output #1: loss = 86.6325 (* 1 = 86.6325 loss)\n",
      "I0411 16:22:49.899492 16153 solver.cpp:189] Iteration 0, loss = 86.8159\n",
      "I0411 16:22:49.899600 16153 solver.cpp:204]     Train net output #0: loss = 86.8159 (* 1 = 86.8159 loss)\n",
      "I0411 16:22:49.899677 16153 solver.cpp:464] Iteration 0, lr = 0.01\n",
      "I0411 16:23:12.265046 16153 solver.cpp:189] Iteration 100, loss = 86.6375\n",
      "I0411 16:23:12.265146 16153 solver.cpp:204]     Train net output #0: loss = 86.6375 (* 1 = 86.6375 loss)\n",
      "I0411 16:23:12.265189 16153 solver.cpp:464] Iteration 100, lr = 0.01\n",
      "I0411 16:23:34.577065 16153 solver.cpp:189] Iteration 200, loss = 85.0672\n",
      "I0411 16:23:34.577250 16153 solver.cpp:204]     Train net output #0: loss = 85.0672 (* 1 = 85.0672 loss)\n",
      "I0411 16:23:34.577286 16153 solver.cpp:464] Iteration 200, lr = 0.01\n",
      "I0411 16:23:57.167243 16153 solver.cpp:189] Iteration 300, loss = 84.5415\n",
      "I0411 16:23:57.167376 16153 solver.cpp:204]     Train net output #0: loss = 84.5415 (* 1 = 84.5415 loss)\n",
      "I0411 16:23:57.167418 16153 solver.cpp:464] Iteration 300, lr = 0.01\n",
      "I0411 16:24:19.610124 16153 solver.cpp:189] Iteration 400, loss = 82.6765\n",
      "I0411 16:24:19.610292 16153 solver.cpp:204]     Train net output #0: loss = 82.6765 (* 1 = 82.6765 loss)\n",
      "I0411 16:24:19.610327 16153 solver.cpp:464] Iteration 400, lr = 0.01\n",
      "I0411 16:24:42.328287 16153 solver.cpp:189] Iteration 500, loss = 81.6134\n",
      "I0411 16:24:42.328414 16153 solver.cpp:204]     Train net output #0: loss = 81.6134 (* 1 = 81.6134 loss)\n",
      "I0411 16:24:42.328452 16153 solver.cpp:464] Iteration 500, lr = 0.01\n",
      "I0411 16:25:05.013552 16153 solver.cpp:189] Iteration 600, loss = 80.6013\n",
      "I0411 16:25:05.013846 16153 solver.cpp:204]     Train net output #0: loss = 80.6013 (* 1 = 80.6013 loss)\n",
      "I0411 16:25:05.013886 16153 solver.cpp:464] Iteration 600, lr = 0.01\n",
      "I0411 16:25:27.472970 16153 solver.cpp:189] Iteration 700, loss = 77.8177\n",
      "I0411 16:25:27.473069 16153 solver.cpp:204]     Train net output #0: loss = 77.8177 (* 1 = 77.8177 loss)\n",
      "I0411 16:25:27.473109 16153 solver.cpp:464] Iteration 700, lr = 0.01\n",
      "I0411 16:25:50.236284 16153 solver.cpp:189] Iteration 800, loss = 78.2394\n",
      "I0411 16:25:50.236546 16153 solver.cpp:204]     Train net output #0: loss = 78.2394 (* 1 = 78.2394 loss)\n",
      "I0411 16:25:50.236593 16153 solver.cpp:464] Iteration 800, lr = 0.01\n",
      "I0411 16:26:12.880128 16153 solver.cpp:189] Iteration 900, loss = 80.3366\n",
      "I0411 16:26:12.880234 16153 solver.cpp:204]     Train net output #0: loss = 80.3366 (* 1 = 80.3366 loss)\n",
      "I0411 16:26:12.880271 16153 solver.cpp:464] Iteration 900, lr = 0.01\n",
      "I0411 16:26:35.384377 16153 solver.cpp:266] Iteration 1000, Testing net (#0)\n",
      "I0411 16:26:36.492604 16153 solver.cpp:315]     Test net output #0: accuracy = 0.1036\n",
      "I0411 16:26:36.492736 16153 solver.cpp:315]     Test net output #1: loss = 78.1312 (* 1 = 78.1312 loss)\n",
      "I0411 16:26:36.713057 16153 solver.cpp:189] Iteration 1000, loss = 77.6676\n",
      "I0411 16:26:36.713171 16153 solver.cpp:204]     Train net output #0: loss = 77.6676 (* 1 = 77.6676 loss)\n",
      "I0411 16:26:36.713214 16153 solver.cpp:464] Iteration 1000, lr = 0.01\n",
      "I0411 16:26:59.519156 16153 solver.cpp:189] Iteration 1100, loss = 78.855\n",
      "I0411 16:26:59.519273 16153 solver.cpp:204]     Train net output #0: loss = 78.855 (* 1 = 78.855 loss)\n",
      "I0411 16:26:59.519312 16153 solver.cpp:464] Iteration 1100, lr = 0.01\n",
      "I0411 16:27:22.002488 16153 solver.cpp:189] Iteration 1200, loss = 71.6152\n",
      "I0411 16:27:22.002787 16153 solver.cpp:204]     Train net output #0: loss = 71.6152 (* 1 = 71.6152 loss)\n",
      "I0411 16:27:22.002823 16153 solver.cpp:464] Iteration 1200, lr = 0.01\n",
      "I0411 16:27:44.652386 16153 solver.cpp:189] Iteration 1300, loss = 74.3217\n",
      "I0411 16:27:44.652529 16153 solver.cpp:204]     Train net output #0: loss = 74.3217 (* 1 = 74.3217 loss)\n",
      "I0411 16:27:44.652590 16153 solver.cpp:464] Iteration 1300, lr = 0.01\n",
      "I0411 16:28:07.365761 16153 solver.cpp:189] Iteration 1400, loss = 72.7249\n",
      "I0411 16:28:07.365974 16153 solver.cpp:204]     Train net output #0: loss = 72.7249 (* 1 = 72.7249 loss)\n",
      "I0411 16:28:07.366016 16153 solver.cpp:464] Iteration 1400, lr = 0.01\n",
      "I0411 16:28:29.945658 16153 solver.cpp:189] Iteration 1500, loss = 68.8428\n",
      "I0411 16:28:29.945791 16153 solver.cpp:204]     Train net output #0: loss = 68.8428 (* 1 = 68.8428 loss)\n",
      "I0411 16:28:29.945830 16153 solver.cpp:464] Iteration 1500, lr = 0.01\n",
      "I0411 16:28:52.749292 16153 solver.cpp:189] Iteration 1600, loss = 71.185\n",
      "I0411 16:28:52.749491 16153 solver.cpp:204]     Train net output #0: loss = 71.185 (* 1 = 71.185 loss)\n",
      "I0411 16:28:52.749532 16153 solver.cpp:464] Iteration 1600, lr = 0.01\n",
      "I0411 16:29:15.264433 16153 solver.cpp:189] Iteration 1700, loss = 67.9476\n",
      "I0411 16:29:15.264562 16153 solver.cpp:204]     Train net output #0: loss = 67.9476 (* 1 = 67.9476 loss)\n",
      "I0411 16:29:15.264605 16153 solver.cpp:464] Iteration 1700, lr = 0.01\n",
      "I0411 16:29:37.859035 16153 solver.cpp:189] Iteration 1800, loss = 67.7816\n",
      "I0411 16:29:37.859181 16153 solver.cpp:204]     Train net output #0: loss = 67.7816 (* 1 = 67.7816 loss)\n",
      "I0411 16:29:37.859225 16153 solver.cpp:464] Iteration 1800, lr = 0.01\n",
      "I0411 16:30:00.581606 16153 solver.cpp:189] Iteration 1900, loss = 69.5029\n",
      "I0411 16:30:00.581784 16153 solver.cpp:204]     Train net output #0: loss = 69.5029 (* 1 = 69.5029 loss)\n",
      "I0411 16:30:00.581830 16153 solver.cpp:464] Iteration 1900, lr = 0.01\n",
      "I0411 16:30:22.823071 16153 solver.cpp:266] Iteration 2000, Testing net (#0)\n",
      "I0411 16:30:23.867805 16153 solver.cpp:315]     Test net output #0: accuracy = 0.1596\n",
      "I0411 16:30:23.867918 16153 solver.cpp:315]     Test net output #1: loss = 73.251 (* 1 = 73.251 loss)\n",
      "I0411 16:30:24.087265 16153 solver.cpp:189] Iteration 2000, loss = 67.6533\n",
      "I0411 16:30:24.087363 16153 solver.cpp:204]     Train net output #0: loss = 67.6533 (* 1 = 67.6533 loss)\n",
      "I0411 16:30:24.087398 16153 solver.cpp:464] Iteration 2000, lr = 0.01\n",
      "I0411 16:30:46.864729 16153 solver.cpp:189] Iteration 2100, loss = 67.3904\n",
      "I0411 16:30:46.864863 16153 solver.cpp:204]     Train net output #0: loss = 67.3904 (* 1 = 67.3904 loss)\n",
      "I0411 16:30:46.864897 16153 solver.cpp:464] Iteration 2100, lr = 0.01\n",
      "I0411 16:31:09.498895 16153 solver.cpp:189] Iteration 2200, loss = 64.3427\n",
      "I0411 16:31:09.499032 16153 solver.cpp:204]     Train net output #0: loss = 64.3427 (* 1 = 64.3427 loss)\n",
      "I0411 16:31:09.499066 16153 solver.cpp:464] Iteration 2200, lr = 0.01\n",
      "I0411 16:31:32.004936 16153 solver.cpp:189] Iteration 2300, loss = 67.0728\n",
      "I0411 16:31:32.005046 16153 solver.cpp:204]     Train net output #0: loss = 67.0728 (* 1 = 67.0728 loss)\n",
      "I0411 16:31:32.005079 16153 solver.cpp:464] Iteration 2300, lr = 0.01\n",
      "I0411 16:31:54.813647 16153 solver.cpp:189] Iteration 2400, loss = 64.7102\n",
      "I0411 16:31:54.813920 16153 solver.cpp:204]     Train net output #0: loss = 64.7102 (* 1 = 64.7102 loss)\n",
      "I0411 16:31:54.813963 16153 solver.cpp:464] Iteration 2400, lr = 0.01\n",
      "I0411 16:32:17.302351 16153 solver.cpp:189] Iteration 2500, loss = 63.229\n",
      "I0411 16:32:17.302479 16153 solver.cpp:204]     Train net output #0: loss = 63.229 (* 1 = 63.229 loss)\n",
      "I0411 16:32:17.302521 16153 solver.cpp:464] Iteration 2500, lr = 0.01\n",
      "I0411 16:32:39.989410 16153 solver.cpp:189] Iteration 2600, loss = 63.098\n",
      "I0411 16:32:39.989701 16153 solver.cpp:204]     Train net output #0: loss = 63.098 (* 1 = 63.098 loss)\n",
      "I0411 16:32:39.989740 16153 solver.cpp:464] Iteration 2600, lr = 0.01\n",
      "I0411 16:33:02.679270 16153 solver.cpp:189] Iteration 2700, loss = 62.6353\n",
      "I0411 16:33:02.679404 16153 solver.cpp:204]     Train net output #0: loss = 62.6353 (* 1 = 62.6353 loss)\n",
      "I0411 16:33:02.679443 16153 solver.cpp:464] Iteration 2700, lr = 0.01\n",
      "I0411 16:33:25.109745 16153 solver.cpp:189] Iteration 2800, loss = 64.7873\n",
      "I0411 16:33:25.109905 16153 solver.cpp:204]     Train net output #0: loss = 64.7873 (* 1 = 64.7873 loss)\n",
      "I0411 16:33:25.109949 16153 solver.cpp:464] Iteration 2800, lr = 0.01\n",
      "I0411 16:33:47.857797 16153 solver.cpp:189] Iteration 2900, loss = 64.3817\n",
      "I0411 16:33:47.857976 16153 solver.cpp:204]     Train net output #0: loss = 64.3817 (* 1 = 64.3817 loss)\n",
      "I0411 16:33:47.858031 16153 solver.cpp:464] Iteration 2900, lr = 0.01\n",
      "I0411 16:34:10.308859 16153 solver.cpp:266] Iteration 3000, Testing net (#0)\n",
      "I0411 16:34:11.340317 16153 solver.cpp:315]     Test net output #0: accuracy = 0.1772\n",
      "I0411 16:34:11.340411 16153 solver.cpp:315]     Test net output #1: loss = 71.7908 (* 1 = 71.7908 loss)\n",
      "I0411 16:34:11.554458 16153 solver.cpp:189] Iteration 3000, loss = 62.1323\n",
      "I0411 16:34:11.554548 16153 solver.cpp:204]     Train net output #0: loss = 62.1323 (* 1 = 62.1323 loss)\n",
      "I0411 16:34:11.554582 16153 solver.cpp:464] Iteration 3000, lr = 0.01\n",
      "I0411 16:34:34.183037 16153 solver.cpp:189] Iteration 3100, loss = 64.2129\n",
      "I0411 16:34:34.183178 16153 solver.cpp:204]     Train net output #0: loss = 64.2129 (* 1 = 64.2129 loss)\n",
      "I0411 16:34:34.183219 16153 solver.cpp:464] Iteration 3100, lr = 0.01\n",
      "I0411 16:34:57.023989 16153 solver.cpp:189] Iteration 3200, loss = 60.9797\n",
      "I0411 16:34:57.024173 16153 solver.cpp:204]     Train net output #0: loss = 60.9797 (* 1 = 60.9797 loss)\n",
      "I0411 16:34:57.024214 16153 solver.cpp:464] Iteration 3200, lr = 0.01\n",
      "I0411 16:35:19.546298 16153 solver.cpp:189] Iteration 3300, loss = 62.5155\n",
      "I0411 16:35:19.546411 16153 solver.cpp:204]     Train net output #0: loss = 62.5155 (* 1 = 62.5155 loss)\n",
      "I0411 16:35:19.546463 16153 solver.cpp:464] Iteration 3300, lr = 0.01\n",
      "I0411 16:35:42.237892 16153 solver.cpp:189] Iteration 3400, loss = 63.4652\n",
      "I0411 16:35:42.238028 16153 solver.cpp:204]     Train net output #0: loss = 63.4652 (* 1 = 63.4652 loss)\n",
      "I0411 16:35:42.238066 16153 solver.cpp:464] Iteration 3400, lr = 0.01\n",
      "I0411 16:36:04.886171 16153 solver.cpp:189] Iteration 3500, loss = 62.6539\n",
      "I0411 16:36:04.886322 16153 solver.cpp:204]     Train net output #0: loss = 62.6539 (* 1 = 62.6539 loss)\n",
      "I0411 16:36:04.886369 16153 solver.cpp:464] Iteration 3500, lr = 0.01\n",
      "I0411 16:36:27.443946 16153 solver.cpp:189] Iteration 3600, loss = 63.2352\n",
      "I0411 16:36:27.444139 16153 solver.cpp:204]     Train net output #0: loss = 63.2352 (* 1 = 63.2352 loss)\n",
      "I0411 16:36:27.444188 16153 solver.cpp:464] Iteration 3600, lr = 0.01\n",
      "I0411 16:36:50.340150 16153 solver.cpp:189] Iteration 3700, loss = 61.0105\n",
      "I0411 16:36:50.340262 16153 solver.cpp:204]     Train net output #0: loss = 61.0105 (* 1 = 61.0105 loss)\n",
      "I0411 16:36:50.340311 16153 solver.cpp:464] Iteration 3700, lr = 0.01\n",
      "I0411 16:37:13.003794 16153 solver.cpp:189] Iteration 3800, loss = 62.6055\n",
      "I0411 16:37:13.003949 16153 solver.cpp:204]     Train net output #0: loss = 62.6055 (* 1 = 62.6055 loss)\n",
      "I0411 16:37:13.003990 16153 solver.cpp:464] Iteration 3800, lr = 0.01\n",
      "I0411 16:37:35.542958 16153 solver.cpp:189] Iteration 3900, loss = 58.6321\n",
      "I0411 16:37:35.543068 16153 solver.cpp:204]     Train net output #0: loss = 58.6321 (* 1 = 58.6321 loss)\n",
      "I0411 16:37:35.543107 16153 solver.cpp:464] Iteration 3900, lr = 0.01\n",
      "I0411 16:37:58.090384 16153 solver.cpp:266] Iteration 4000, Testing net (#0)\n",
      "I0411 16:37:59.130769 16153 solver.cpp:315]     Test net output #0: accuracy = 0.2188\n",
      "I0411 16:37:59.130887 16153 solver.cpp:315]     Test net output #1: loss = 68.0034 (* 1 = 68.0034 loss)\n",
      "I0411 16:37:59.347713 16153 solver.cpp:189] Iteration 4000, loss = 60.3321\n",
      "I0411 16:37:59.347810 16153 solver.cpp:204]     Train net output #0: loss = 60.3321 (* 1 = 60.3321 loss)\n",
      "I0411 16:37:59.347846 16153 solver.cpp:464] Iteration 4000, lr = 0.01\n",
      "I0411 16:38:21.918624 16153 solver.cpp:189] Iteration 4100, loss = 61.7325\n",
      "I0411 16:38:21.918740 16153 solver.cpp:204]     Train net output #0: loss = 61.7325 (* 1 = 61.7325 loss)\n",
      "I0411 16:38:21.918776 16153 solver.cpp:464] Iteration 4100, lr = 0.01\n",
      "I0411 16:38:44.641211 16153 solver.cpp:189] Iteration 4200, loss = 58.2828\n",
      "I0411 16:38:44.641728 16153 solver.cpp:204]     Train net output #0: loss = 58.2828 (* 1 = 58.2828 loss)\n",
      "I0411 16:38:44.641957 16153 solver.cpp:464] Iteration 4200, lr = 0.01\n",
      "I0411 16:39:07.329120 16153 solver.cpp:189] Iteration 4300, loss = 61.5238\n",
      "I0411 16:39:07.329237 16153 solver.cpp:204]     Train net output #0: loss = 61.5238 (* 1 = 61.5238 loss)\n",
      "I0411 16:39:07.329274 16153 solver.cpp:464] Iteration 4300, lr = 0.01\n",
      "I0411 16:39:29.845572 16153 solver.cpp:189] Iteration 4400, loss = 55.2341\n",
      "I0411 16:39:29.845736 16153 solver.cpp:204]     Train net output #0: loss = 55.2341 (* 1 = 55.2341 loss)\n",
      "I0411 16:39:29.845770 16153 solver.cpp:464] Iteration 4400, lr = 0.01\n",
      "I0411 16:39:52.666580 16153 solver.cpp:189] Iteration 4500, loss = 60.5513\n",
      "I0411 16:39:52.666749 16153 solver.cpp:204]     Train net output #0: loss = 60.5513 (* 1 = 60.5513 loss)\n",
      "I0411 16:39:52.666791 16153 solver.cpp:464] Iteration 4500, lr = 0.01\n",
      "I0411 16:40:15.183051 16153 solver.cpp:189] Iteration 4600, loss = 63.2076\n",
      "I0411 16:40:15.183229 16153 solver.cpp:204]     Train net output #0: loss = 63.2076 (* 1 = 63.2076 loss)\n",
      "I0411 16:40:15.183269 16153 solver.cpp:464] Iteration 4600, lr = 0.01\n",
      "I0411 16:40:37.823983 16153 solver.cpp:189] Iteration 4700, loss = 60.7868\n",
      "I0411 16:40:37.824102 16153 solver.cpp:204]     Train net output #0: loss = 60.7868 (* 1 = 60.7868 loss)\n",
      "I0411 16:40:37.824143 16153 solver.cpp:464] Iteration 4700, lr = 0.01\n",
      "I0411 16:41:00.518151 16153 solver.cpp:189] Iteration 4800, loss = 56.6536\n",
      "I0411 16:41:00.518317 16153 solver.cpp:204]     Train net output #0: loss = 56.6536 (* 1 = 56.6536 loss)\n",
      "I0411 16:41:00.518357 16153 solver.cpp:464] Iteration 4800, lr = 0.01\n",
      "I0411 16:41:22.941615 16153 solver.cpp:189] Iteration 4900, loss = 60.4998\n",
      "I0411 16:41:22.941704 16153 solver.cpp:204]     Train net output #0: loss = 60.4998 (* 1 = 60.4998 loss)\n",
      "I0411 16:41:22.941742 16153 solver.cpp:464] Iteration 4900, lr = 0.01\n",
      "I0411 16:41:45.373450 16153 solver.cpp:266] Iteration 5000, Testing net (#0)\n",
      "I0411 16:41:46.430492 16153 solver.cpp:315]     Test net output #0: accuracy = 0.2092\n",
      "I0411 16:41:46.430594 16153 solver.cpp:315]     Test net output #1: loss = 68.7744 (* 1 = 68.7744 loss)\n",
      "I0411 16:41:46.655406 16153 solver.cpp:189] Iteration 5000, loss = 56.0187\n",
      "I0411 16:41:46.655547 16153 solver.cpp:204]     Train net output #0: loss = 56.0187 (* 1 = 56.0187 loss)\n",
      "I0411 16:41:46.655586 16153 solver.cpp:464] Iteration 5000, lr = 0.001\n",
      "I0411 16:42:09.344104 16153 solver.cpp:189] Iteration 5100, loss = 36.1859\n",
      "I0411 16:42:09.344208 16153 solver.cpp:204]     Train net output #0: loss = 36.1859 (* 1 = 36.1859 loss)\n",
      "I0411 16:42:09.344254 16153 solver.cpp:464] Iteration 5100, lr = 0.001\n",
      "I0411 16:42:31.978296 16153 solver.cpp:189] Iteration 5200, loss = 27.2797\n",
      "I0411 16:42:31.978500 16153 solver.cpp:204]     Train net output #0: loss = 27.2797 (* 1 = 27.2797 loss)\n",
      "I0411 16:42:31.978543 16153 solver.cpp:464] Iteration 5200, lr = 0.001\n",
      "I0411 16:42:54.830236 16153 solver.cpp:189] Iteration 5300, loss = 28.0127\n",
      "I0411 16:42:54.830355 16153 solver.cpp:204]     Train net output #0: loss = 28.0127 (* 1 = 28.0127 loss)\n",
      "I0411 16:42:54.830390 16153 solver.cpp:464] Iteration 5300, lr = 0.001\n",
      "I0411 16:43:17.375217 16153 solver.cpp:189] Iteration 5400, loss = 30.5772\n",
      "I0411 16:43:17.375370 16153 solver.cpp:204]     Train net output #0: loss = 30.5772 (* 1 = 30.5772 loss)\n",
      "I0411 16:43:17.375411 16153 solver.cpp:464] Iteration 5400, lr = 0.001\n",
      "I0411 16:43:40.053308 16153 solver.cpp:189] Iteration 5500, loss = 31.2673\n",
      "I0411 16:43:40.053406 16153 solver.cpp:204]     Train net output #0: loss = 31.2673 (* 1 = 31.2673 loss)\n",
      "I0411 16:43:40.053443 16153 solver.cpp:464] Iteration 5500, lr = 0.001\n",
      "I0411 16:44:02.675169 16153 solver.cpp:189] Iteration 5600, loss = 30.6217\n",
      "I0411 16:44:02.675660 16153 solver.cpp:204]     Train net output #0: loss = 30.6217 (* 1 = 30.6217 loss)\n",
      "I0411 16:44:02.675714 16153 solver.cpp:464] Iteration 5600, lr = 0.001\n",
      "I0411 16:44:25.152559 16153 solver.cpp:189] Iteration 5700, loss = 25.8143\n",
      "I0411 16:44:25.152690 16153 solver.cpp:204]     Train net output #0: loss = 25.8143 (* 1 = 25.8143 loss)\n",
      "I0411 16:44:25.152729 16153 solver.cpp:464] Iteration 5700, lr = 0.001\n",
      "I0411 16:44:47.823993 16153 solver.cpp:189] Iteration 5800, loss = 27.092\n",
      "I0411 16:44:47.824214 16153 solver.cpp:204]     Train net output #0: loss = 27.092 (* 1 = 27.092 loss)\n",
      "I0411 16:44:47.824256 16153 solver.cpp:464] Iteration 5800, lr = 0.001\n",
      "I0411 16:45:10.413651 16153 solver.cpp:189] Iteration 5900, loss = 30.0294\n",
      "I0411 16:45:10.413763 16153 solver.cpp:204]     Train net output #0: loss = 30.0294 (* 1 = 30.0294 loss)\n",
      "I0411 16:45:10.413804 16153 solver.cpp:464] Iteration 5900, lr = 0.001\n",
      "I0411 16:45:32.647593 16153 solver.cpp:266] Iteration 6000, Testing net (#0)\n",
      "I0411 16:45:33.749023 16153 solver.cpp:315]     Test net output #0: accuracy = 0.418\n",
      "I0411 16:45:33.749110 16153 solver.cpp:315]     Test net output #1: loss = 50.4979 (* 1 = 50.4979 loss)\n",
      "I0411 16:45:33.964354 16153 solver.cpp:189] Iteration 6000, loss = 28.272\n",
      "I0411 16:45:33.964447 16153 solver.cpp:204]     Train net output #0: loss = 28.272 (* 1 = 28.272 loss)\n",
      "I0411 16:45:33.964479 16153 solver.cpp:464] Iteration 6000, lr = 0.001\n",
      "I0411 16:45:56.652742 16153 solver.cpp:189] Iteration 6100, loss = 31.6799\n",
      "I0411 16:45:56.652894 16153 solver.cpp:204]     Train net output #0: loss = 31.6799 (* 1 = 31.6799 loss)\n",
      "I0411 16:45:56.652932 16153 solver.cpp:464] Iteration 6100, lr = 0.001\n",
      "I0411 16:46:19.095000 16153 solver.cpp:189] Iteration 6200, loss = 26.5021\n",
      "I0411 16:46:19.095226 16153 solver.cpp:204]     Train net output #0: loss = 26.5021 (* 1 = 26.5021 loss)\n",
      "I0411 16:46:19.095259 16153 solver.cpp:464] Iteration 6200, lr = 0.001\n",
      "I0411 16:46:41.739362 16153 solver.cpp:189] Iteration 6300, loss = 28.9045\n",
      "I0411 16:46:41.739526 16153 solver.cpp:204]     Train net output #0: loss = 28.9045 (* 1 = 28.9045 loss)\n",
      "I0411 16:46:41.739567 16153 solver.cpp:464] Iteration 6300, lr = 0.001\n",
      "I0411 16:47:04.420860 16153 solver.cpp:189] Iteration 6400, loss = 29.3767\n",
      "I0411 16:47:04.421031 16153 solver.cpp:204]     Train net output #0: loss = 29.3767 (* 1 = 29.3767 loss)\n",
      "I0411 16:47:04.421066 16153 solver.cpp:464] Iteration 6400, lr = 0.001\n",
      "I0411 16:47:26.898712 16153 solver.cpp:189] Iteration 6500, loss = 30.7102\n",
      "I0411 16:47:26.898821 16153 solver.cpp:204]     Train net output #0: loss = 30.7102 (* 1 = 30.7102 loss)\n",
      "I0411 16:47:26.898870 16153 solver.cpp:464] Iteration 6500, lr = 0.001\n",
      "I0411 16:47:49.668288 16153 solver.cpp:189] Iteration 6600, loss = 25.5089\n",
      "I0411 16:47:49.668778 16153 solver.cpp:204]     Train net output #0: loss = 25.5089 (* 1 = 25.5089 loss)\n",
      "I0411 16:47:49.668823 16153 solver.cpp:464] Iteration 6600, lr = 0.001\n",
      "I0411 16:48:12.241900 16153 solver.cpp:189] Iteration 6700, loss = 27.6598\n",
      "I0411 16:48:12.242010 16153 solver.cpp:204]     Train net output #0: loss = 27.6598 (* 1 = 27.6598 loss)\n",
      "I0411 16:48:12.242044 16153 solver.cpp:464] Iteration 6700, lr = 0.001\n",
      "I0411 16:48:34.903277 16153 solver.cpp:189] Iteration 6800, loss = 28.2213\n",
      "I0411 16:48:34.903460 16153 solver.cpp:204]     Train net output #0: loss = 28.2213 (* 1 = 28.2213 loss)\n",
      "I0411 16:48:34.903499 16153 solver.cpp:464] Iteration 6800, lr = 0.001\n",
      "I0411 16:48:57.683192 16153 solver.cpp:189] Iteration 6900, loss = 29.9847\n",
      "I0411 16:48:57.683329 16153 solver.cpp:204]     Train net output #0: loss = 29.9847 (* 1 = 29.9847 loss)\n",
      "I0411 16:48:57.683369 16153 solver.cpp:464] Iteration 6900, lr = 0.001\n",
      "I0411 16:49:19.917042 16153 solver.cpp:266] Iteration 7000, Testing net (#0)\n",
      "I0411 16:49:20.967326 16153 solver.cpp:315]     Test net output #0: accuracy = 0.4132\n",
      "I0411 16:49:20.967403 16153 solver.cpp:315]     Test net output #1: loss = 50.7524 (* 1 = 50.7524 loss)\n",
      "I0411 16:49:21.181651 16153 solver.cpp:189] Iteration 7000, loss = 24.3321\n",
      "I0411 16:49:21.181745 16153 solver.cpp:204]     Train net output #0: loss = 24.3321 (* 1 = 24.3321 loss)\n",
      "I0411 16:49:21.181777 16153 solver.cpp:464] Iteration 7000, lr = 0.001\n",
      "I0411 16:49:43.489996 16153 solver.cpp:189] Iteration 7100, loss = 25.126\n",
      "I0411 16:49:43.490105 16153 solver.cpp:204]     Train net output #0: loss = 25.126 (* 1 = 25.126 loss)\n",
      "I0411 16:49:43.490142 16153 solver.cpp:464] Iteration 7100, lr = 0.001\n",
      "I0411 16:50:05.980582 16153 solver.cpp:189] Iteration 7200, loss = 26.9876\n",
      "I0411 16:50:05.980967 16153 solver.cpp:204]     Train net output #0: loss = 26.9876 (* 1 = 26.9876 loss)\n",
      "I0411 16:50:05.981009 16153 solver.cpp:464] Iteration 7200, lr = 0.001\n",
      "I0411 16:50:28.434314 16153 solver.cpp:189] Iteration 7300, loss = 30.1815\n",
      "I0411 16:50:28.434471 16153 solver.cpp:204]     Train net output #0: loss = 30.1815 (* 1 = 30.1815 loss)\n",
      "I0411 16:50:28.434510 16153 solver.cpp:464] Iteration 7300, lr = 0.001\n",
      "I0411 16:50:51.296692 16153 solver.cpp:189] Iteration 7400, loss = 27.0653\n",
      "I0411 16:50:51.296880 16153 solver.cpp:204]     Train net output #0: loss = 27.0653 (* 1 = 27.0653 loss)\n",
      "I0411 16:50:51.296917 16153 solver.cpp:464] Iteration 7400, lr = 0.001\n",
      "I0411 16:51:13.859894 16153 solver.cpp:189] Iteration 7500, loss = 24.5067\n",
      "I0411 16:51:13.859990 16153 solver.cpp:204]     Train net output #0: loss = 24.5067 (* 1 = 24.5067 loss)\n",
      "I0411 16:51:13.860023 16153 solver.cpp:464] Iteration 7500, lr = 0.001\n",
      "I0411 16:51:36.418241 16153 solver.cpp:189] Iteration 7600, loss = 23.6618\n",
      "I0411 16:51:36.418393 16153 solver.cpp:204]     Train net output #0: loss = 23.6618 (* 1 = 23.6618 loss)\n",
      "I0411 16:51:36.418433 16153 solver.cpp:464] Iteration 7600, lr = 0.001\n",
      "I0411 16:51:59.205114 16153 solver.cpp:189] Iteration 7700, loss = 28.7774\n",
      "I0411 16:51:59.205248 16153 solver.cpp:204]     Train net output #0: loss = 28.7774 (* 1 = 28.7774 loss)\n",
      "I0411 16:51:59.205291 16153 solver.cpp:464] Iteration 7700, lr = 0.001\n",
      "I0411 16:52:21.744240 16153 solver.cpp:189] Iteration 7800, loss = 26.8362\n",
      "I0411 16:52:21.744436 16153 solver.cpp:204]     Train net output #0: loss = 26.8362 (* 1 = 26.8362 loss)\n",
      "I0411 16:52:21.744474 16153 solver.cpp:464] Iteration 7800, lr = 0.001\n",
      "I0411 16:52:44.438215 16153 solver.cpp:189] Iteration 7900, loss = 28.8753\n",
      "I0411 16:52:44.438364 16153 solver.cpp:204]     Train net output #0: loss = 28.8753 (* 1 = 28.8753 loss)\n",
      "I0411 16:52:44.438406 16153 solver.cpp:464] Iteration 7900, lr = 0.001\n",
      "I0411 16:53:06.893652 16153 solver.cpp:266] Iteration 8000, Testing net (#0)\n",
      "I0411 16:53:07.941839 16153 solver.cpp:315]     Test net output #0: accuracy = 0.4288\n",
      "I0411 16:53:07.942039 16153 solver.cpp:315]     Test net output #1: loss = 49.3367 (* 1 = 49.3367 loss)\n",
      "I0411 16:53:08.156714 16153 solver.cpp:189] Iteration 8000, loss = 23.8458\n",
      "I0411 16:53:08.156823 16153 solver.cpp:204]     Train net output #0: loss = 23.8458 (* 1 = 23.8458 loss)\n",
      "I0411 16:53:08.156859 16153 solver.cpp:464] Iteration 8000, lr = 0.001\n",
      "I0411 16:53:30.646865 16153 solver.cpp:189] Iteration 8100, loss = 27.9211\n",
      "I0411 16:53:30.646952 16153 solver.cpp:204]     Train net output #0: loss = 27.9211 (* 1 = 27.9211 loss)\n",
      "I0411 16:53:30.646986 16153 solver.cpp:464] Iteration 8100, lr = 0.001\n",
      "I0411 16:53:53.449617 16153 solver.cpp:189] Iteration 8200, loss = 27.293\n",
      "I0411 16:53:53.449933 16153 solver.cpp:204]     Train net output #0: loss = 27.293 (* 1 = 27.293 loss)\n",
      "I0411 16:53:53.449977 16153 solver.cpp:464] Iteration 8200, lr = 0.001\n",
      "I0411 16:54:15.985831 16153 solver.cpp:189] Iteration 8300, loss = 28.6895\n",
      "I0411 16:54:15.985947 16153 solver.cpp:204]     Train net output #0: loss = 28.6895 (* 1 = 28.6895 loss)\n",
      "I0411 16:54:15.985990 16153 solver.cpp:464] Iteration 8300, lr = 0.001\n",
      "I0411 16:54:38.659566 16153 solver.cpp:189] Iteration 8400, loss = 24.5386\n",
      "I0411 16:54:38.659759 16153 solver.cpp:204]     Train net output #0: loss = 24.5386 (* 1 = 24.5386 loss)\n",
      "I0411 16:54:38.659800 16153 solver.cpp:464] Iteration 8400, lr = 0.001\n",
      "I0411 16:55:01.438164 16153 solver.cpp:189] Iteration 8500, loss = 26.6278\n",
      "I0411 16:55:01.438285 16153 solver.cpp:204]     Train net output #0: loss = 26.6278 (* 1 = 26.6278 loss)\n",
      "I0411 16:55:01.438324 16153 solver.cpp:464] Iteration 8500, lr = 0.001\n",
      "I0411 16:55:23.823812 16153 solver.cpp:189] Iteration 8600, loss = 26.837\n",
      "I0411 16:55:23.824079 16153 solver.cpp:204]     Train net output #0: loss = 26.837 (* 1 = 26.837 loss)\n",
      "I0411 16:55:23.824285 16153 solver.cpp:464] Iteration 8600, lr = 0.001\n",
      "I0411 16:55:46.535759 16153 solver.cpp:189] Iteration 8700, loss = 28.2616\n",
      "I0411 16:55:46.535919 16153 solver.cpp:204]     Train net output #0: loss = 28.2616 (* 1 = 28.2616 loss)\n",
      "I0411 16:55:46.535966 16153 solver.cpp:464] Iteration 8700, lr = 0.001\n",
      "I0411 16:56:09.244415 16153 solver.cpp:189] Iteration 8800, loss = 22.3859\n",
      "I0411 16:56:09.244606 16153 solver.cpp:204]     Train net output #0: loss = 22.3859 (* 1 = 22.3859 loss)\n",
      "I0411 16:56:09.244653 16153 solver.cpp:464] Iteration 8800, lr = 0.001\n",
      "I0411 16:56:31.747803 16153 solver.cpp:189] Iteration 8900, loss = 26.4929\n",
      "I0411 16:56:31.747932 16153 solver.cpp:204]     Train net output #0: loss = 26.4929 (* 1 = 26.4929 loss)\n",
      "I0411 16:56:31.747970 16153 solver.cpp:464] Iteration 8900, lr = 0.001\n",
      "I0411 16:56:54.325075 16153 solver.cpp:266] Iteration 9000, Testing net (#0)\n",
      "I0411 16:56:55.382926 16153 solver.cpp:315]     Test net output #0: accuracy = 0.4036\n",
      "I0411 16:56:55.383023 16153 solver.cpp:315]     Test net output #1: loss = 51.2966 (* 1 = 51.2966 loss)\n",
      "I0411 16:56:55.599792 16153 solver.cpp:189] Iteration 9000, loss = 25.6641\n",
      "I0411 16:56:55.599899 16153 solver.cpp:204]     Train net output #0: loss = 25.6641 (* 1 = 25.6641 loss)\n",
      "I0411 16:56:55.599933 16153 solver.cpp:464] Iteration 9000, lr = 0.001\n",
      "I0411 16:57:18.085156 16153 solver.cpp:189] Iteration 9100, loss = 27.4542\n",
      "I0411 16:57:18.085266 16153 solver.cpp:204]     Train net output #0: loss = 27.4542 (* 1 = 27.4542 loss)\n",
      "I0411 16:57:18.085300 16153 solver.cpp:464] Iteration 9100, lr = 0.001\n",
      "I0411 16:57:40.769429 16153 solver.cpp:189] Iteration 9200, loss = 26.7496\n",
      "I0411 16:57:40.769569 16153 solver.cpp:204]     Train net output #0: loss = 26.7496 (* 1 = 26.7496 loss)\n",
      "I0411 16:57:40.769606 16153 solver.cpp:464] Iteration 9200, lr = 0.001\n",
      "I0411 16:58:03.536244 16153 solver.cpp:189] Iteration 9300, loss = 23.7357\n",
      "I0411 16:58:03.536397 16153 solver.cpp:204]     Train net output #0: loss = 23.7357 (* 1 = 23.7357 loss)\n",
      "I0411 16:58:03.536440 16153 solver.cpp:464] Iteration 9300, lr = 0.001\n",
      "I0411 16:58:26.096272 16153 solver.cpp:189] Iteration 9400, loss = 24.0989\n",
      "I0411 16:58:26.096441 16153 solver.cpp:204]     Train net output #0: loss = 24.0989 (* 1 = 24.0989 loss)\n",
      "I0411 16:58:26.096475 16153 solver.cpp:464] Iteration 9400, lr = 0.001\n",
      "I0411 16:58:48.875571 16153 solver.cpp:189] Iteration 9500, loss = 27.3723\n",
      "I0411 16:58:48.875715 16153 solver.cpp:204]     Train net output #0: loss = 27.3723 (* 1 = 27.3723 loss)\n",
      "I0411 16:58:48.875757 16153 solver.cpp:464] Iteration 9500, lr = 0.001\n",
      "I0411 16:59:11.561024 16153 solver.cpp:189] Iteration 9600, loss = 26.5195\n",
      "I0411 16:59:11.561213 16153 solver.cpp:204]     Train net output #0: loss = 26.5195 (* 1 = 26.5195 loss)\n",
      "I0411 16:59:11.561251 16153 solver.cpp:464] Iteration 9600, lr = 0.001\n",
      "I0411 16:59:34.050822 16153 solver.cpp:189] Iteration 9700, loss = 26.6022\n",
      "I0411 16:59:34.050952 16153 solver.cpp:204]     Train net output #0: loss = 26.6022 (* 1 = 26.6022 loss)\n",
      "I0411 16:59:34.050989 16153 solver.cpp:464] Iteration 9700, lr = 0.001\n",
      "I0411 16:59:56.893754 16153 solver.cpp:189] Iteration 9800, loss = 24.1304\n",
      "I0411 16:59:56.894011 16153 solver.cpp:204]     Train net output #0: loss = 24.1304 (* 1 = 24.1304 loss)\n",
      "I0411 16:59:56.894054 16153 solver.cpp:464] Iteration 9800, lr = 0.001\n",
      "I0411 17:00:19.384969 16153 solver.cpp:189] Iteration 9900, loss = 26.6297\n",
      "I0411 17:00:19.385092 16153 solver.cpp:204]     Train net output #0: loss = 26.6297 (* 1 = 26.6297 loss)\n",
      "I0411 17:00:19.385220 16153 solver.cpp:464] Iteration 9900, lr = 0.001\n",
      "I0411 17:00:41.914636 16153 solver.cpp:334] Snapshotting to rdlclass/data/train_iter_10000.caffemodel\n",
      "I0411 17:00:41.958745 16153 solver.cpp:342] Snapshotting solver state to rdlclass/data/train_iter_10000.solverstate\n",
      "I0411 17:00:42.121167 16153 solver.cpp:248] Iteration 10000, loss = 26.0466\n",
      "I0411 17:00:42.121261 16153 solver.cpp:266] Iteration 10000, Testing net (#0)\n",
      "I0411 17:00:43.206985 16153 solver.cpp:315]     Test net output #0: accuracy = 0.4204\n",
      "I0411 17:00:43.207069 16153 solver.cpp:315]     Test net output #1: loss = 50.3757 (* 1 = 50.3757 loss)\n",
      "I0411 17:00:43.207098 16153 solver.cpp:253] Optimization Done.\n",
      "I0411 17:00:43.207121 16153 caffe.cpp:134] Optimization Done.\n"
     ]
    }
   ],
   "source": [
    "!caffe/build/tools/caffe train -solver rdlclass/solver.prototxt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for (index, image) in enumerate(Xt_resized):\n",
    "    Xt_resized[index] = image.reshape(30, 30, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(30, 30, 1)\n"
     ]
    }
   ],
   "source": [
    "print Xt_resized[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(30, 30, 1)\n",
      "(30, 30, 1)\n",
      "(30, 30, 1)\n",
      "(30, 30, 1)\n",
      "(30, 30, 1)\n",
      "(30, 30, 1)\n",
      "(30, 30, 1)\n",
      "(30, 30, 1)\n",
      "(30, 30, 1)\n",
      "(30, 30, 1)\n"
     ]
    }
   ],
   "source": [
    "for image in X_test[:10]:\n",
    "    new_image = image[:, :, np.newaxis]\n",
    "    print new_image.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "prediction shape: (2000,)\n",
      "predicted class: 895\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXsAAAEACAYAAABS29YJAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSZIkSfn8H5nrRTFVHlsvAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f51b8f48850>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Set the right path to your model definition file, pretrained model weights,\n",
    "# and the image you would like to classify.\n",
    "MODEL_FILE = 'rdlclass/deploy.prototxt'\n",
    "PRETRAINED = 'rdlclass/data/train_iter_10000.caffemodel'\n",
    "\n",
    "caffe.set_mode_cpu()\n",
    "net = caffe.Classifier(MODEL_FILE, PRETRAINED)\n",
    "\n",
    "batch_size = 20000\n",
    "predictions = []\n",
    "for i in range(len(Xt_resized) / batch_size):\n",
    "    print(i)\n",
    "    prediction = net.predict([image.reshape(1, 1, 900) for image in Xt_resized[i * (batch_size): (i + 1) * batch_size]]) # predict takes any number of images, and formats them for the Caffe net automatically\n",
    "    predictions.append(prediction)\n",
    "\n",
    "print 'prediction shape:', prediction[0].shape\n",
    "plt.plot(prediction[0])\n",
    "print 'predicted class:', prediction[0].argmax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20000, 2000)\n"
     ]
    }
   ],
   "source": [
    "print prediction.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(200000,)\n"
     ]
    }
   ],
   "source": [
    "labels = np.array([p.argmax() for batch in predictions for p in batch])\n",
    "# labels = prediction.argmax(axis=1)\n",
    "print labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4135"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1.0 * sum(labels == y_test) / len(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data = {'Id':range(len(labels)), 'Prediction':labels})\n",
    "df.to_csv('pred.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Id,Prediction\r\n",
      "0,1469\r\n",
      "1,1380\r\n",
      "2,1563\r\n",
      "3,1654\r\n",
      "4,1637\r\n",
      "5,621\r\n",
      "6,1351\r\n",
      "7,1132\r\n",
      "8,808\r\n"
     ]
    }
   ],
   "source": [
    "!head \"pred.csv\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
